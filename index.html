<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Unsupervised 3D Pose Transfer with Cross Consistency and Dual Reconstruction</title>
	<meta property="og:image" content="./files/3dpt.gif"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Unsupervised 3D Pose Transfer with Cross Consistency and Dual Reconstruction" />
	<meta property="og:description" content="Transfer the pose from source mesh to target mesh." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Unsupervised 3D Pose Transfer with Cross Consistency and Dual Reconstruction</span><br>
		<span style="font-size:25px;line-height:2.0">Submitted to TPAMI</span><br>
		<table align=center width=1100px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=250px>
						<center>
							<span style="font-size:22px"><a href="https://chaoyuesong.github.io">Chaoyue Song</a></span>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							<span style="font-size:22px">Jiacheng Wei</span>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							<span style="font-size:22px"><a href="https://scholar.google.com/citations?user=qtGY5T4AAAAJ&hl">Ruibo Li</a></span>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							<span style="font-size:22px"><a href="https://sites.google.com/site/fayaoliu/">Fayao Liu</a></span>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							<span style="font-size:22px"><a href="https://guosheng.github.io/">Guosheng Lin</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=360px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2211.10278'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/ChaoyueSong/X-DualNet'>[GitHub]</a></span><br>
						</center>
					</td>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=850px>
					<center>
						<img class="round" style="width:850px" src="./misc/fig1.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>
<br>
	<hr>

	<table align=center width=900px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				The goal of 3D pose transfer is to transfer the pose from the source mesh to the target mesh while preserving the identity information (e.g., face, body shape) of the target mesh. Deep learning-based methods improved the efficiency and performance of 3D pose transfer. However, most of them are trained under the supervision of the ground truth, whose availability is limited in real-world scenarios. In this work, we present X-DualNet, a simple yet effective approach that enables unsupervised 3D pose transfer. In X-DualNet, we introduce a generator <math> <mi>G</mi> </math> which contains correspondence learning and pose transfer modules to achieve 3D pose transfer. We learn the shape correspondence by solving an optimal transport problem without any key point annotations and generate high-quality meshes with our elastic instance normalization (ElaIN) in the pose transfer module. With <math> <mi>G</mi> </math> as the basic component, we propose a cross consistency learning scheme and a dual reconstruction objective to learn the pose transfer without supervision. Besides that, we also adopt an as-rigid-as-possible deformer in the training process to fine-tune the body shape of the generated results. Extensive experiments on human and animal data demonstrate that our framework can successfully achieve comparable performance as the state-of-the-art supervised approaches.
			</td>
		</tr>
	</table>
	<br>
	<hr>
	
	
	<center><h1>Method</h1></center>
        In X-DualNet, we introduce a generator G that contains correspondence learning and pose transfer modules to
achieve the 3D pose transfer. We learn the shape correspondence by solving an optimal transport problem without any
key point annotations and generate high-quality meshes
with our proposed elastic instance normalization in the pose
transfer module. With the generator G as the basic component, we
propose a cross consistency learning scheme and a dual
reconstruction objective to learn the pose transfer without supervision. Besides that, we also adopt an as-rigid-as-possible
deformer to fine-tune the body shape of the generated results. <br><br>
	<table align=center width=900px>
		<tr>
			<td align=center width=900px>
				<center>
					<td><img class="round" style="width:900px" src="./misc/network.png"/></td>
				</center>
			</td>
		</tr>
	</table>

	<br>
	<hr>
	
	<center><h1>Comparison with other methods on human meshes</h1></center>
	The identity and pose meshes are from SMPL. Our method and 3D-CoreNet can generate better results than NPT and the proposed unsupervised
baseline. The surfaces of meshes generated by NPT are not always smooth. And the proposed unsupervised baseline cannot preserve the body
shapes very well. <br><br>
	<table align=center width=900px>
  		<tr>
			<td width=900px>
				<center>
					<td><img class="round" style="width:900px" src="./misc/fig4.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
        <hr>
	
	<center><h1>Comparison with other methods on animal meshes</h1></center>
	The identity and pose meshes are from SMAL. Our method and 3D-CoreNet produce satisfactory results on the animal data. NPT produces many
artifacts and cannot transfer the pose successfully. The proposed baseline sometimes cannot preserve the shape identity (e.g., the tail) well. <br><br>
	<table align=center width=900px>
  		<tr>
			<td width=900px>
				<center>
					<td><img class="round" style="width:900px" src="./misc/fig5.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
        <hr>
	
	<center><h1>Ablation studies</h1></center>
	we study the effectiveness of ElaIN, ARAP deformer, and backward correspondence loss in our model on human data.   <br><br>
	<table align=center width=500px>
  		<tr>
			<td width=500px>
				<center>
					<td><img class="round" style="width:500px" src="./misc/fig6.png"/></td>
				</center>
			</td>
		</tr>
	</table>
        When we
replace our ElaIN with SPAdaIN, the surface of the mesh
has clear artifacts and is not smooth.
	<br>
	<hr>
	<table align=center width=900px>
  		<tr>
			<td width=900px>
				<center>
					<td><img class="round" style="width:900px" src="./misc/fig7.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	When we do not add the ARAP deformer in the training loop, the generated
results do not preserve the body shape well which can be shown in the green bounding boxes. In the third column, the body shape is sunken from the
left and right sides. When we remove the backward correspondence loss in the training, the pose transfer results are not accurate which can
be shown in the red bounding boxes. In the fourth column, the right arm is farther from the body than others.
	<br>
        <hr>
	
	<table align=center width=450px>
		<center><h1>Paper</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./misc/paper.png"/></a></td>
			<td><span style="font-size:14pt">C. Song, J. Wei, R. Li, F. Liu, G. Lin.<br>
				<b>Unsupervised 3D Pose Transfer with Cross Consistency and Dual Reconstruction.</b><br>
				arXiv, 2022.<br>
				(hosted on <a href="https://arxiv.org/abs/2211.10278">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
        <table align=center width=600px>
	<tr>
		<td><span style="font-size:14pt"><center>
			<a href="./misc/bibtex.txt">[Bibtex]</a>
		</center></td>
	</tr>
	</table>
	<br>
	<hr>
        	<table align=center width=900px>
		<center><h1>Related projects</h1></center>
		<tr>
	<td>
	    <a href="https://chaoyuesong.github.io/3d-corenet/"> 3D Pose Transfer with Correspondence Learning and Mesh Refinement. NeurIPS 2021.</a> <br>
	    <a href="https://jiashunwang.github.io/Neural-Pose-Transfer/"> Neural Pose Transfer by Spatially Adaptive Instance Normalization. CVPR 2020.</a> <br>
	    </td>
		</tr>
	</table>
	<br>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

